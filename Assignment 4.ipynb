{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e19148",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Networks (41 marks total)\n",
    "### Due: November 19 at 11:59pm (grace period until November 21 at 11:59pm)\n",
    "\n",
    "### Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e109f9",
   "metadata": {},
   "source": [
    "The goal of this assignment is to apply Recurrent Neural Networks (RNNs) in PyTorch for text data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17652ac",
   "metadata": {},
   "source": [
    "## Part 1: LSTM\n",
    "\n",
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ab773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef88ec-e3ff-4d49-8539-7c9deba1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbb6fc",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Preprocessing (12 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b2831",
   "metadata": {},
   "source": [
    "For this assignment, we will be using the imdb dataset from the ðŸ¤— Datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37934a3-e866-4eb3-bc0e-8d35e0c2daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load the dataset (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daadfd2",
   "metadata": {},
   "source": [
    "We need to preprocess the data before we can feed it into the model. The first step is to define a custom tokenizer to perform the following tasks: \n",
    "- Extract the text data from the dataset\n",
    "- Remove any non-alphanumeric characters\n",
    "- Separate each data sample into separate words (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68381a25-1033-4416-a5bc-9d0d16ba4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data_iter):\n",
    "    '''Tokenizes the input data\n",
    "    input: data_iter (type: dictionary)\n",
    "    output: text (type: list[list[str]])\n",
    "    '''\n",
    "    # TO DO: fill in this function (2 marks)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4468c1a",
   "metadata": {},
   "source": [
    "We will also need to extract the labels from the dataset. Complete the label_extractor function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4753bf7-0519-45d1-9820-d71d531fd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(data_iter):\n",
    "    '''Takes the label for each data sample and stores it in a separate list\n",
    "    input: data_iter (type: dictionary)\n",
    "    output: labels (type: list)\n",
    "    '''\n",
    "    # TO DO: fill in this function (1 mark)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852c910",
   "metadata": {},
   "source": [
    "Now that we have the text data separated into words, we need to define the vocabulary. We cannot keep all the words in the vocabulary, so we want to limit the vocabulary size and only take the most common words. In this case, the maximum vocabulary size is 10,000 words. Any word that is excluded will be set to an unknown token. You can use the function below to build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02163e32-5265-4165-83c5-7f0cf99aa7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary\n",
    "def build_vocab(data_iter, max_size=10000):\n",
    "    '''Creates a vocabulary based on the training data\n",
    "    input: data_iter (type: list[list[str]])\n",
    "    output: vocab (type: dictionary)\n",
    "    '''\n",
    "    counter = Counter()\n",
    "    for words in data_iter:\n",
    "        counter.update(words)\n",
    "    # Filter to most common words\n",
    "    vocab = {word: i + 1 for i, (word, _) in enumerate(counter.most_common(max_size))}\n",
    "    # Add a token for unknown words (0)\n",
    "    vocab['<unk>'] = 0 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428e6c4",
   "metadata": {},
   "source": [
    "In the vocabulary, each word is mapped to a number in the vocabulary. We will need to encode the dataset based on these numbers, as tensors cannot handle string data.\n",
    "\n",
    "The next step is to pad or truncate each sequence based on a maximum length, to make sure that the dataset can be transformed into a tensor (as discussed in class).\n",
    "\n",
    "Fill in the function below to encode and pad the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbced45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(text, vocab, max_len=100):\n",
    "    '''Encode and pad the input text dataset\n",
    "    input: text (type: list[list[str]])\n",
    "    input: vocab (type: dictionary)\n",
    "    input: max_len (type: int)\n",
    "    output: texts (type: list[list[str]])\n",
    "    '''\n",
    "    # TO DO: fill in the function to encode text to integers and pad/truncate sequences (2 marks)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171dcbf",
   "metadata": {},
   "source": [
    "The next step is to create a custom PyTorch Dataset class that calls the `encode_and_pad()` function and stores the text and labels as tensors. Fill in the `init` portion of the class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        # TO DO: call the encode_and_pad() function and set self.texts and self.labels (2 marks)\n",
    "        pass\n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx): \n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a5bb9",
   "metadata": {},
   "source": [
    "Now you can call all the functions that have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b86df-7c52-4f1c-967d-c8787a818608",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256 # Sequence length\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# TO DO: Tokenize training data (1 mark)\n",
    "\n",
    "# TO DO: Extract labels from training and testing data (1 mark)\n",
    "\n",
    "# TO DO: Build Vocabulary (from training data only) (1 mark)\n",
    "\n",
    "# TO DO: Prepare datasets (using TextDataset class) and store datasets using DataLoaders (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645c862",
   "metadata": {},
   "source": [
    "### Step 2: Define Model (4 marks)\n",
    "\n",
    "For this assignment, we will be using the LSTM model. Inside the LSTM model, the first layer will be an embedding layer, to convert the singular numerical representation of each word into an embedded vector. We can use `nn.Embedding(...)` for this.\n",
    "\n",
    "Define LSTMClassifier below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define LSTM class (4 marks)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        # TO DO: Embedding layer \n",
    "        pass\n",
    "        # TO DO: LSTM layer\n",
    "         \n",
    "        # TO DO: Linear fully-connected layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TO DO: Fill in the model steps\n",
    "        # NOTE: The LSTM outputs (output, (hidden, cell)) - hidden and cell are not used\n",
    "        # NOTE: Use the hidden state from the final time step for the fc layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaae02b",
   "metadata": {},
   "source": [
    "### Step 3: Define Training and Testing Loops (4 marks)\n",
    "\n",
    "The next step is to define functions for the training and testing loops. For this case, we will only be calculating the loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbed46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define training loop (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define testing loop (2 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7263d5",
   "metadata": {},
   "source": [
    "### Step 4: Train and Evaluate (3 marks)\n",
    "\n",
    "Now that we have all the necessary functions, we can select our hyperparameters, and train and evaluate our model. For this case, since we are not comparing different models, we do not need a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1 # Binary classification\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create model object (1 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0853152",
   "metadata": {},
   "source": [
    "Since this case is binary optimization, we will use the binary cross entropy criterion, `BCEWithLogitsLoss()`. This model is similar to Cross Entropy, but uses a sigmoid layer instead of a softmax layer. For the optimization function, we will use Adam with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define optimization model and criterion (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7e3ea",
   "metadata": {},
   "source": [
    "We can now run our training and testing loops. Since this takes a long time to run, we will set the number of epochs to 5. Print out the training and testing losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a177fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Run training and testing loops and print losses for each epoch (1 mark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab79c",
   "metadata": {},
   "source": [
    "## Part 2: Questions and Process Description\n",
    "\n",
    "### Questions (12 marks)\n",
    "\n",
    "1. Do you think this model worked well to classify the data? Why or why not? Can you make a good decision about this only using loss data?\n",
    "1. What could you do to further improve the results? Provide two suggestions.\n",
    "1. Why does a simple RNN often underperform compared to LSTM or GRU on long text sequences such as IMDB reviews?\n",
    "1. Why does the embedding layer improve performance compared to one-hot encoding?\n",
    "1. If we switched to character-level input instead of word-level, what changes would we expect in performance and training time?\n",
    "1. How does vocabulary size influence model performance and generalization?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3494f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dce273",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322ea90",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d91cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
