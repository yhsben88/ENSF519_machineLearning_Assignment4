{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e19148",
   "metadata": {},
   "source": [
    "# Assignment 4: Recurrent Neural Networks (41 marks total)\n",
    "### Due: November 19 at 11:59pm (grace period until November 21 at 11:59pm)\n",
    "\n",
    "### Name: Hiu Sum Yuen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e109f9",
   "metadata": {},
   "source": [
    "The goal of this assignment is to apply Recurrent Neural Networks (RNNs) in PyTorch for text data classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17652ac",
   "metadata": {},
   "source": [
    "## Part 1: LSTM\n",
    "\n",
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fd8ab773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f0ef88ec-e3ff-4d49-8539-7c9deba1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbb6fc",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Preprocessing (12 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b2831",
   "metadata": {},
   "source": [
    "For this assignment, we will be using the imdb dataset from the ðŸ¤— Datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d37934a3-e866-4eb3-bc0e-8d35e0c2daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Load the dataset (1 mark)\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daadfd2",
   "metadata": {},
   "source": [
    "We need to preprocess the data before we can feed it into the model. The first step is to define a custom tokenizer to perform the following tasks: \n",
    "- Extract the text data from the dataset\n",
    "- Remove any non-alphanumeric characters\n",
    "- Separate each data sample into separate words (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "68381a25-1033-4416-a5bc-9d0d16ba4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data_iter):\n",
    "    '''Tokenizes the input data\n",
    "    input: data_iter (type: dictionary)\n",
    "    output: text (type: list[list[str]])\n",
    "    '''\n",
    "    # TO DO: fill in this function (2 marks)\n",
    "    text_data = []\n",
    "    for text in data_iter['text']:\n",
    "        # Remove non-alphanumeric characters and convert to lowercase\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\\\s]', '', text.lower())\n",
    "        # Split into tokens (words)\n",
    "        tokens = cleaned_text.split()\n",
    "        text_data.append(tokens)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4468c1a",
   "metadata": {},
   "source": [
    "We will also need to extract the labels from the dataset. Complete the label_extractor function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c4753bf7-0519-45d1-9820-d71d531fd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extractor(data_iter):\n",
    "    '''Takes the label for each data sample and stores it in a separate list\n",
    "    input: data_iter (type: dictionary)\n",
    "    output: labels (type: list)\n",
    "    '''\n",
    "    # TO DO: fill in this function (1 mark)\n",
    "    return [label for label in data_iter['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852c910",
   "metadata": {},
   "source": [
    "Now that we have the text data separated into words, we need to define the vocabulary. We cannot keep all the words in the vocabulary, so we want to limit the vocabulary size and only take the most common words. In this case, the maximum vocabulary size is 10,000 words. Any word that is excluded will be set to an unknown token. You can use the function below to build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "02163e32-5265-4165-83c5-7f0cf99aa7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary\n",
    "def build_vocab(data_iter, max_size=20000):\n",
    "    '''Creates a vocabulary based on the training data\n",
    "    input: data_iter (type: list[list[str]])\n",
    "    output: vocab (type: dictionary)\n",
    "    '''\n",
    "    counter = Counter()\n",
    "    for words in data_iter:\n",
    "        counter.update(words)\n",
    "    # Filter to most common words\n",
    "    vocab = {word: i + 1 for i, (word, _) in enumerate(counter.most_common(max_size))}\n",
    "    # Add a token for unknown words (0)\n",
    "    vocab['<unk>'] = 0 \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428e6c4",
   "metadata": {},
   "source": [
    "In the vocabulary, each word is mapped to a number in the vocabulary. We will need to encode the dataset based on these numbers, as tensors cannot handle string data.\n",
    "\n",
    "The next step is to pad or truncate each sequence based on a maximum length, to make sure that the dataset can be transformed into a tensor (as discussed in class).\n",
    "\n",
    "Fill in the function below to encode and pad the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dbced45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(text, vocab, max_len=300):\n",
    "    '''Encode and pad the input text dataset\n",
    "    input: text (type: list[list[str]])\n",
    "    input: vocab (type: dictionary)\n",
    "    input: max_len (type: int)\n",
    "    output: texts (type: list[list[str]])\n",
    "    '''\n",
    "    # TO DO: fill in the function to encode text to integers and pad/truncate sequences (2 marks)\n",
    "    encoded_texts = []\n",
    "    for tokens in text:\n",
    "        # Encode tokens to integers\n",
    "        encoded = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "        # Pad or truncate\n",
    "        if len(encoded) < max_len:\n",
    "            # Pad with zeros\n",
    "            padded = encoded + [0] * (max_len - len(encoded))\n",
    "        else:\n",
    "            # Truncate\n",
    "            padded = encoded[:max_len]\n",
    "        encoded_texts.append(padded)\n",
    "    return encoded_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171dcbf",
   "metadata": {},
   "source": [
    "The next step is to create a custom PyTorch Dataset class that calls the `encode_and_pad()` function and stores the text and labels as tensors. Fill in the `init` portion of the class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df8a8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom PyTorch Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        # TO DO: call the encode_and_pad() function and set self.texts and self.labels (2 marks)\n",
    "        self.texts = torch.tensor(encode_and_pad(texts, vocab, max_len), dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx): \n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a5bb9",
   "metadata": {},
   "source": [
    "Now you can call all the functions that have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "930b86df-7c52-4f1c-967d-c8787a818608",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256 # Sequence length\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# TO DO: Tokenize training data (1 mark)\n",
    "train_text = tokenizer(dataset['train'])\n",
    "test_text = tokenizer(dataset['test'])\n",
    "# TO DO: Extract labels from training and testing data (1 mark)\n",
    "train_labels = label_extractor(dataset['train'])\n",
    "test_labels = label_extractor(dataset['test'])\n",
    "# TO DO: Build Vocabulary (from training data only) (1 mark)\n",
    "vocab = build_vocab(train_text)\n",
    "# TO DO: Prepare datasets (using TextDataset class) and store datasets using DataLoaders (1 mark)\n",
    "train_dataset = TextDataset(train_text, train_labels, vocab, MAX_LEN)\n",
    "test_dataset = TextDataset(test_text, test_labels, vocab, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645c862",
   "metadata": {},
   "source": [
    "### Step 2: Define Model (4 marks)\n",
    "\n",
    "For this assignment, we will be using the LSTM model. Inside the LSTM model, the first layer will be an embedding layer, to convert the singular numerical representation of each word into an embedded vector. We can use `nn.Embedding(...)` for this.\n",
    "\n",
    "Define LSTMClassifier below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6e26b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define LSTM class (4 marks)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        # TO DO: Embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # TO DO: LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        # TO DO: Linear fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TO DO: Fill in the model steps\n",
    "        # NOTE: The LSTM outputs (output, (hidden, cell)) - hidden and cell are not used\n",
    "        # NOTE: Use the hidden state from the final time step for the fc layer\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        # Use the hidden state from the final time step\n",
    "        last_hidden = hidden[-1]\n",
    "        # Fully connected layer\n",
    "        output = self.fc(last_hidden)\n",
    "        # Squeeze the output to remove the extra dimension for BCEWithLogitsLoss\n",
    "        return output.squeeze(1)  # This changes [batch_size, 1] to [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaae02b",
   "metadata": {},
   "source": [
    "### Step 3: Define Training and Testing Loops (4 marks)\n",
    "\n",
    "The next step is to define functions for the training and testing loops. For this case, we will only be calculating the loss at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3cbed46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define training loop (2 marks)\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  # Now output shape is [batch_size]\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "43d2804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define testing loop (2 marks)\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.float().to(device)\n",
    "            \n",
    "            # Get predictions (already squeezed in model forward)\n",
    "            logits = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert logits to probabilities and then to binary predictions\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).long()\n",
    "\n",
    "            # Compute accuracy\n",
    "            correct += (preds == target.long()).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(test_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7263d5",
   "metadata": {},
   "source": [
    "### Step 4: Train and Evaluate (3 marks)\n",
    "\n",
    "Now that we have all the necessary functions, we can select our hyperparameters, and train and evaluate our model. For this case, since we are not comparing different models, we do not need a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6745f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1 # Binary classification\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dbbeae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create model object (1 mark)\n",
    "model = LSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ff16437a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(20001, 100, padding_idx=0)\n",
       "  (lstm): LSTM(100, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0853152",
   "metadata": {},
   "source": [
    "Since this case is binary optimization, we will use the binary cross entropy criterion, `BCEWithLogitsLoss()`. This model is similar to Cross Entropy, but uses a sigmoid layer instead of a softmax layer. For the optimization function, we will use Adam with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "264c1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define optimization model and criterion (1 mark)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7e3ea",
   "metadata": {},
   "source": [
    "We can now run our training and testing loops. Since this takes a long time to run, we will set the number of epochs to 5. Print out the training and testing losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a7a177fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.6932, Test Loss: 0.6932, Test Accuracy: 0.5000\n",
      "Epoch 2/5, Train Loss: 0.6932, Test Loss: 0.6932, Test Accuracy: 0.5000\n",
      "Epoch 3/5, Train Loss: 0.6932, Test Loss: 0.6932, Test Accuracy: 0.5000\n",
      "Epoch 4/5, Train Loss: 0.6932, Test Loss: 0.6931, Test Accuracy: 0.5000\n",
      "Epoch 5/5, Train Loss: 0.6932, Test Loss: 0.6931, Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Run training and testing loops and print losses for each epoch (1 mark)\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab79c",
   "metadata": {},
   "source": [
    "## Part 2: Questions and Process Description\n",
    "\n",
    "### Questions (12 marks)\n",
    "\n",
    "1. Do you think this model worked well to classify the data? Why or why not? Can you make a good decision about this only using loss data?\n",
    "    1. With Loss at 0.69 without dropping, the model shows that it is not learning and effectively performing like random select.\n",
    "2. What could you do to further improve the results? Provide two suggestions.\n",
    "    1. Use bidirectional LSTM to capture context from both directions.\n",
    "    2. Use pre-trained word embeddings (like GloVe or Word2Vec) instead of training embeddings from scratch.\n",
    "3. Why does a simple RNN often underperform compared to LSTM or GRU on long text sequences such as IMDB reviews?\n",
    "    1. Simple RNNs suffer from learning long-range dependencies in long sequences. LSTMs and GRUs have gating mechanisms (forget, input, output gates) that allow them to better preserve and control information flow over long sequences.\n",
    "4. Why does the embedding layer improve performance compared to one-hot encoding?\n",
    "    1. Embedding layers learn dense, continuous vector representations that capture semantic relationships between words, while one-hot encoding creates sparse, high-dimensional vectors with no meaningful relationships between different words. Embeddings also have much lower dimensionality and can generalize better.\n",
    "5. If we switched to character-level input instead of word-level, what changes would we expect in performance and training time?\n",
    "    1. Character-level models would have longer sequences, increasing training time significantly. Performance might decrease initially due to the increased complexity of learning from characters. Though we may find use in niche cases in better performance on out-of-vocabulary words.\n",
    "6. How does vocabulary size influence model performance and generalization?\n",
    "Larger vocabulary sizes can capture more nuanced language but require more parameters and training data. Smaller vocabularies are more computationally efficient but may lose important semantic information. There's a trade-off - too small and you lose information, too large and you may overfit or require excessive resources.\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3494f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code? DeepSeek\n",
    "1. In what order did you complete the steps? in order, then fix bugs\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not? why am i wrong, what does this do, what does this mean if i do this.\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful? a lot of troubles with having the model learn, instead of a non learning model. I had to trial and error with debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dce273",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322ea90",
   "metadata": {},
   "source": [
    "## Part 3: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "i disliked that i did not understand much of what the problem was, even though i understand RNN conceptually in lecture. I found this assignment frustrating and confusing because i could write code without error but the model wasn't learning so i knew i was doing something wrong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
